{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Drive settings, Imports and Control panel"
      ],
      "metadata": {
        "id": "OBALUUsriGis"
      },
      "id": "OBALUUsriGis"
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda para ver cuanta ram tienes\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')\n",
        "\n",
        "print(ram_gb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-BLLPBH344k",
        "outputId": "e0dbae1f-d999-473c-a8f5-0019a71ac6d3"
      },
      "id": "D-BLLPBH344k",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n",
            "89.639657472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf13ftIPyGjr",
        "outputId": "b4819b88-74a1-4341-c4f2-3a51552ed31e"
      },
      "id": "bf13ftIPyGjr",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#! pip install tensorflow"
      ],
      "metadata": {
        "id": "nicWy31JZXBL"
      },
      "id": "nicWy31JZXBL",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dced684f",
      "metadata": {
        "id": "dced684f"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import json\n",
        "\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Flatten, Input, Concatenate\n",
        "from keras.regularizers import l2\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.resnet import ResNet50\n",
        "from keras.applications.resnet import ResNet\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import skimage.io\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.layers as layers\n",
        "from keras.layers import concatenate\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "import urllib.request\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "# CONTROL PANEL\n",
        "#######################################################\n",
        "\n",
        "target_var = \"hdi\" # either (\"hdi\" or \"gdp\")\n",
        "\n",
        "# Programmed but un-implemented as of yet\n",
        "train_VGG = True\n",
        "train_ResNet = True\n",
        "train_AlexNet = True"
      ],
      "metadata": {
        "id": "7D319E0TiF05"
      },
      "id": "7D319E0TiF05",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A. Loading and preprocessing data"
      ],
      "metadata": {
        "id": "hFX_i6dIhN7Q"
      },
      "id": "hFX_i6dIhN7Q"
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading X_img, X_popn and Y\n",
        "\n",
        "features_img = np.load(\"/content/drive/MyDrive/Thesis/imageset_500_500.npy\") \n",
        "features_img = features_img.astype('float32')\n",
        "features_img = features_img/255.0\n",
        "\n",
        "features_popn = popn_inputs = pd.read_csv(\"/content/drive/MyDrive/Thesis/siei_processed_v1.csv\")[[\"2019_figure_est\"]]\n",
        "\n",
        "features = np.array(list(zip(features_img, np.array(features_popn))))\n",
        "\n",
        "if target_var == 'hdi':\n",
        "    labels = np.load(\"/content/drive/MyDrive/Thesis/target_HDI.npy\") \n",
        "    \n",
        "elif target_var == 'gdp':\n",
        "    labels = np.load(\"/content/drive/MyDrive/Thesis/target_logGDP.npy\") \n",
        "\n",
        "# Split 60% 20% 20%\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features, labels, test_size=0.4, random_state=1945, stratify=labels)\n",
        "\n",
        "# Further split the training set into training and validation sets\n",
        "X_test, X_val, y_test, y_val = train_test_split(\n",
        "    X_test, y_test, test_size=0.5, random_state=1945, stratify=y_test)\n",
        "\n",
        "# Float conversion to allow normalization\n",
        "#X_train=X_train.astype('float32')\n",
        "#X_test=X_test.astype('float32')\n",
        "#X_val=X_val.astype('float32')\n",
        "\n",
        "# Normalization \n",
        "#X_train=X_train/255.0\n",
        "#X_test=X_test/255.0\n",
        "#X_val=X_val/255.0\n",
        "\n",
        "# Convert the numerical labels to one-hot encoded format\n",
        "num_classes = 4\n",
        "y_train_onehot = keras.utils.to_categorical((y_train-1), num_classes=num_classes)\n",
        "y_val_onehot = keras.utils.to_categorical((y_val-1), num_classes=num_classes)\n",
        "y_test_onehot = keras.utils.to_categorical((y_test-1), num_classes=num_classes)"
      ],
      "metadata": {
        "id": "LnTDzGM_iGlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86a679a2-949c-43a8-a653-f8d14e5f7d99"
      },
      "id": "LnTDzGM_iGlC",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-266376720643>:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  features = np.array(list(zip(features_img, np.array(features_popn))))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_img = X_train[:,0]\n",
        "X_val_img = X_val[:,0]\n",
        "X_test_img = X_test[:,0]\n",
        "\n",
        "X_train_popn = X_train[:,1]\n",
        "X_val_popn = X_val[:,1]\n",
        "X_test_popn = X_test[:,1]"
      ],
      "metadata": {
        "id": "vi-gPQBf0n5J"
      },
      "id": "vi-gPQBf0n5J",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Giving appropriate shapes to the training, validation and test inputs\n",
        "# Transform from arrays of arrays (invalid) to unified array\n",
        "\n",
        "# Image inputs\n",
        "X_train_img_0 = np.zeros((3075,500,500,3))\n",
        "for i, element in enumerate(X_train_img):\n",
        "    X_train_img_0[i] = element\n",
        "\n",
        "X_val_img_0 = np.zeros((1026,500,500,3))\n",
        "for i, element in enumerate(X_val_img):\n",
        "    X_val_img_0[i] = element\n",
        "\n",
        "X_test_img_0 = np.zeros((1025,500,500,3))\n",
        "for i, element in enumerate(X_test_img):\n",
        "    X_test_img_0[i] = element\n",
        "\n",
        "# Population figure inputs\n",
        "X_train_popn_0 = np.zeros((3075,))\n",
        "for i, element in enumerate(X_train_popn):\n",
        "    X_train_popn_0[i] = element\n",
        "\n",
        "X_val_popn_0 = np.zeros((1026,))\n",
        "for i, element in enumerate(X_val_popn):\n",
        "    X_val_popn_0[i] = element\n",
        "\n",
        "X_test_popn_0 = np.zeros((1025,))\n",
        "for i, element in enumerate(X_test_popn):\n",
        "    X_test_popn_0[i] = element"
      ],
      "metadata": {
        "id": "JnK7pqMj_ZXJ"
      },
      "id": "JnK7pqMj_ZXJ",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. VGG16"
      ],
      "metadata": {
        "id": "4-kEJpNJhyNi"
      },
      "id": "4-kEJpNJhyNi"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4bd80315",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bd80315",
        "outputId": "8b60a643-0955-4540-ed39-490b0f2dd052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 500, 500, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " block1_conv1 (Conv2D)          (None, 500, 500, 64  1792        ['input_2[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block1_conv2 (Conv2D)          (None, 500, 500, 64  36928       ['block1_conv1[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block1_pool (MaxPooling2D)     (None, 250, 250, 64  0           ['block1_conv2[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block2_conv1 (Conv2D)          (None, 250, 250, 12  73856       ['block1_pool[0][0]']            \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " block2_conv2 (Conv2D)          (None, 250, 250, 12  147584      ['block2_conv1[0][0]']           \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " block2_pool (MaxPooling2D)     (None, 125, 125, 12  0           ['block2_conv2[0][0]']           \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " block3_conv1 (Conv2D)          (None, 125, 125, 25  295168      ['block2_pool[0][0]']            \n",
            "                                6)                                                                \n",
            "                                                                                                  \n",
            " block3_conv2 (Conv2D)          (None, 125, 125, 25  590080      ['block3_conv1[0][0]']           \n",
            "                                6)                                                                \n",
            "                                                                                                  \n",
            " block3_conv3 (Conv2D)          (None, 125, 125, 25  590080      ['block3_conv2[0][0]']           \n",
            "                                6)                                                                \n",
            "                                                                                                  \n",
            " block3_pool (MaxPooling2D)     (None, 62, 62, 256)  0           ['block3_conv3[0][0]']           \n",
            "                                                                                                  \n",
            " block4_conv1 (Conv2D)          (None, 62, 62, 512)  1180160     ['block3_pool[0][0]']            \n",
            "                                                                                                  \n",
            " block4_conv2 (Conv2D)          (None, 62, 62, 512)  2359808     ['block4_conv1[0][0]']           \n",
            "                                                                                                  \n",
            " block4_conv3 (Conv2D)          (None, 62, 62, 512)  2359808     ['block4_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block4_pool (MaxPooling2D)     (None, 31, 31, 512)  0           ['block4_conv3[0][0]']           \n",
            "                                                                                                  \n",
            " block5_conv1 (Conv2D)          (None, 31, 31, 512)  2359808     ['block4_pool[0][0]']            \n",
            "                                                                                                  \n",
            " block5_conv2 (Conv2D)          (None, 31, 31, 512)  2359808     ['block5_conv1[0][0]']           \n",
            "                                                                                                  \n",
            " block5_conv3 (Conv2D)          (None, 31, 31, 512)  2359808     ['block5_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block5_pool (MaxPooling2D)     (None, 15, 15, 512)  0           ['block5_conv3[0][0]']           \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 115200)       0           ['block5_pool[0][0]']            \n",
            "                                                                                                  \n",
            " integer_input (InputLayer)     [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 115201)       0           ['flatten_1[0][0]',              \n",
            "                                                                  'integer_input[0][0]']          \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 256)          29491712    ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 128)          32896       ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 64)           8256        ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 32)           2080        ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 16)           528         ['dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 4)            68          ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 44,250,228\n",
            "Trainable params: 29,535,540\n",
            "Non-trainable params: 14,714,688\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "49/49 [==============================] - 36s 384ms/step - loss: 32.7664 - accuracy: 0.2881 - val_loss: 7.3329 - val_accuracy: 0.3002\n",
            "Epoch 2/100\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 7.5245 - accuracy: 0.3593 - val_loss: 13.7080 - val_accuracy: 0.2329\n",
            "Epoch 3/100\n",
            "49/49 [==============================] - 13s 265ms/step - loss: 6.0517 - accuracy: 0.3954 - val_loss: 2.5565 - val_accuracy: 0.4493\n",
            "Epoch 4/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 4.2248 - accuracy: 0.4153 - val_loss: 2.8722 - val_accuracy: 0.4493\n",
            "Epoch 5/100\n",
            "49/49 [==============================] - 13s 265ms/step - loss: 3.7984 - accuracy: 0.4403 - val_loss: 2.2154 - val_accuracy: 0.5019\n",
            "Epoch 6/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 2.8492 - accuracy: 0.4741 - val_loss: 2.9226 - val_accuracy: 0.4357\n",
            "Epoch 7/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 1.9969 - accuracy: 0.5366 - val_loss: 7.6900 - val_accuracy: 0.3236\n",
            "Epoch 8/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 1.9609 - accuracy: 0.5672 - val_loss: 2.4804 - val_accuracy: 0.5058\n",
            "Epoch 9/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 1.9230 - accuracy: 0.5535 - val_loss: 4.2328 - val_accuracy: 0.4659\n",
            "Epoch 10/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 2.3216 - accuracy: 0.5285 - val_loss: 5.8293 - val_accuracy: 0.3314\n",
            "Epoch 11/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 1.9324 - accuracy: 0.5447 - val_loss: 4.1870 - val_accuracy: 0.3801\n",
            "Epoch 12/100\n",
            "49/49 [==============================] - 13s 270ms/step - loss: 1.7091 - accuracy: 0.5906 - val_loss: 1.7499 - val_accuracy: 0.5058\n",
            "Epoch 13/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.9188 - accuracy: 0.6937 - val_loss: 3.2716 - val_accuracy: 0.4064\n",
            "Epoch 14/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 1.7052 - accuracy: 0.5714 - val_loss: 1.8684 - val_accuracy: 0.5107\n",
            "Epoch 15/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.8896 - accuracy: 0.6940 - val_loss: 0.9069 - val_accuracy: 0.6501\n",
            "Epoch 16/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.8496 - accuracy: 0.7073 - val_loss: 7.2106 - val_accuracy: 0.2914\n",
            "Epoch 17/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 2.4732 - accuracy: 0.5450 - val_loss: 1.0315 - val_accuracy: 0.5828\n",
            "Epoch 18/100\n",
            "49/49 [==============================] - 13s 265ms/step - loss: 0.8117 - accuracy: 0.7060 - val_loss: 1.4988 - val_accuracy: 0.5175\n",
            "Epoch 19/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.8800 - accuracy: 0.7213 - val_loss: 0.8435 - val_accuracy: 0.6452\n",
            "Epoch 20/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 0.7298 - accuracy: 0.7421 - val_loss: 2.1192 - val_accuracy: 0.4366\n",
            "Epoch 21/100\n",
            "49/49 [==============================] - 13s 269ms/step - loss: 0.7533 - accuracy: 0.7327 - val_loss: 1.9985 - val_accuracy: 0.5010\n",
            "Epoch 22/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 0.7415 - accuracy: 0.7259 - val_loss: 0.8678 - val_accuracy: 0.6559\n",
            "Epoch 23/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.6465 - accuracy: 0.7639 - val_loss: 1.5584 - val_accuracy: 0.5117\n",
            "Epoch 24/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 1.0724 - accuracy: 0.6608 - val_loss: 1.3281 - val_accuracy: 0.5780\n",
            "Epoch 25/100\n",
            "49/49 [==============================] - 13s 269ms/step - loss: 0.5483 - accuracy: 0.7977 - val_loss: 0.8645 - val_accuracy: 0.6657\n",
            "Epoch 26/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.5252 - accuracy: 0.8055 - val_loss: 2.7449 - val_accuracy: 0.4172\n",
            "Epoch 27/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 1.2767 - accuracy: 0.6283 - val_loss: 2.1277 - val_accuracy: 0.5614\n",
            "Epoch 28/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 0.7252 - accuracy: 0.7496 - val_loss: 1.9588 - val_accuracy: 0.5205\n",
            "Epoch 29/100\n",
            "49/49 [==============================] - 13s 269ms/step - loss: 0.9435 - accuracy: 0.6992 - val_loss: 2.1911 - val_accuracy: 0.4854\n",
            "Epoch 30/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 0.7531 - accuracy: 0.7200 - val_loss: 1.6519 - val_accuracy: 0.5721\n",
            "Epoch 31/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.7565 - accuracy: 0.7327 - val_loss: 0.8263 - val_accuracy: 0.6686\n",
            "Epoch 32/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 0.4820 - accuracy: 0.8215 - val_loss: 1.3045 - val_accuracy: 0.5721\n",
            "Epoch 33/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.5152 - accuracy: 0.8137 - val_loss: 1.5271 - val_accuracy: 0.5390\n",
            "Epoch 34/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 0.6139 - accuracy: 0.7802 - val_loss: 1.3548 - val_accuracy: 0.6160\n",
            "Epoch 35/100\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 0.4952 - accuracy: 0.8218 - val_loss: 1.6976 - val_accuracy: 0.5595\n",
            "Epoch 36/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.5367 - accuracy: 0.8104 - val_loss: 1.1988 - val_accuracy: 0.6150\n",
            "Epoch 37/100\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 0.6314 - accuracy: 0.7815 - val_loss: 3.9227 - val_accuracy: 0.3733\n",
            "Epoch 38/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 1.5108 - accuracy: 0.5932 - val_loss: 3.6971 - val_accuracy: 0.3723\n",
            "Epoch 39/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 1.4231 - accuracy: 0.6013 - val_loss: 1.6604 - val_accuracy: 0.5312\n",
            "Epoch 40/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.9969 - accuracy: 0.6849 - val_loss: 0.9503 - val_accuracy: 0.6277\n",
            "Epoch 41/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 0.8947 - accuracy: 0.7220 - val_loss: 7.3313 - val_accuracy: 0.3392\n",
            "Epoch 42/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 1.3862 - accuracy: 0.6472 - val_loss: 0.9930 - val_accuracy: 0.5789\n",
            "Epoch 43/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.5339 - accuracy: 0.8026 - val_loss: 1.1236 - val_accuracy: 0.5663\n",
            "Epoch 44/100\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 0.5077 - accuracy: 0.8049 - val_loss: 0.8588 - val_accuracy: 0.6667\n",
            "Epoch 45/100\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 0.3934 - accuracy: 0.8452 - val_loss: 1.0036 - val_accuracy: 0.6218\n",
            "Epoch 46/100\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 0.4044 - accuracy: 0.8504 - val_loss: 1.1497 - val_accuracy: 0.5936\n",
            "Epoch 47/100\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 0.9152 - accuracy: 0.7015 - val_loss: 0.9317 - val_accuracy: 0.6413\n",
            "Epoch 48/100\n",
            "49/49 [==============================] - 13s 269ms/step - loss: 0.3999 - accuracy: 0.8602 - val_loss: 0.9321 - val_accuracy: 0.6326\n",
            "Epoch 49/100\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 0.5618 - accuracy: 0.8088 - val_loss: 1.3015 - val_accuracy: 0.5302\n",
            "Epoch 50/100\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 0.6488 - accuracy: 0.7763 - val_loss: 0.8709 - val_accuracy: 0.6637\n",
            "Epoch 51/100\n",
            "49/49 [==============================] - 13s 265ms/step - loss: 0.4348 - accuracy: 0.8520 - val_loss: 0.8563 - val_accuracy: 0.6579\n",
            "Epoch 52/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.3469 - accuracy: 0.8842 - val_loss: 1.8001 - val_accuracy: 0.5283\n",
            "Epoch 53/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 0.7339 - accuracy: 0.7694 - val_loss: 1.1462 - val_accuracy: 0.5926\n",
            "Epoch 54/100\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 0.3670 - accuracy: 0.8693 - val_loss: 1.5664 - val_accuracy: 0.5478\n",
            "Epoch 55/100\n",
            "49/49 [==============================] - 13s 265ms/step - loss: 0.4262 - accuracy: 0.8494 - val_loss: 1.3806 - val_accuracy: 0.5994\n",
            "Epoch 56/100\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 1.3752 - accuracy: 0.6598 - val_loss: 1.9767 - val_accuracy: 0.5780\n",
            "Epoch 57/100\n",
            "49/49 [==============================] - 13s 269ms/step - loss: 0.9287 - accuracy: 0.7099 - val_loss: 1.3844 - val_accuracy: 0.5858\n",
            "Epoch 58/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.9211 - accuracy: 0.7200 - val_loss: 1.2954 - val_accuracy: 0.5458\n",
            "Epoch 59/100\n",
            "49/49 [==============================] - 13s 265ms/step - loss: 0.4711 - accuracy: 0.8429 - val_loss: 0.9423 - val_accuracy: 0.6511\n",
            "Epoch 60/100\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 0.4000 - accuracy: 0.8650 - val_loss: 0.9389 - val_accuracy: 0.6199\n",
            "Epoch 61/100\n",
            "49/49 [==============================] - 13s 269ms/step - loss: 0.3976 - accuracy: 0.8634 - val_loss: 0.8986 - val_accuracy: 0.6277\n",
            "Epoch 62/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 0.5485 - accuracy: 0.8192 - val_loss: 4.6823 - val_accuracy: 0.3821\n",
            "Epoch 63/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 1.0736 - accuracy: 0.6810 - val_loss: 2.7343 - val_accuracy: 0.3879\n",
            "Epoch 64/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 0.6120 - accuracy: 0.7974 - val_loss: 0.8146 - val_accuracy: 0.6657\n",
            "Epoch 65/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 0.3795 - accuracy: 0.8767 - val_loss: 1.1392 - val_accuracy: 0.6004\n",
            "Epoch 66/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 0.4639 - accuracy: 0.8416 - val_loss: 0.9949 - val_accuracy: 0.6189\n",
            "Epoch 67/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.3515 - accuracy: 0.8833 - val_loss: 0.9438 - val_accuracy: 0.6501\n",
            "Epoch 68/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 0.2617 - accuracy: 0.9180 - val_loss: 1.2963 - val_accuracy: 0.5887\n",
            "Epoch 69/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.7422 - accuracy: 0.7623 - val_loss: 1.2943 - val_accuracy: 0.5283\n",
            "Epoch 70/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 0.5146 - accuracy: 0.8185 - val_loss: 0.8392 - val_accuracy: 0.6442\n",
            "Epoch 71/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.2656 - accuracy: 0.9239 - val_loss: 0.9499 - val_accuracy: 0.6686\n",
            "Epoch 72/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 0.3437 - accuracy: 0.8855 - val_loss: 1.1311 - val_accuracy: 0.6014\n",
            "Epoch 73/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 0.2881 - accuracy: 0.9024 - val_loss: 4.2475 - val_accuracy: 0.5097\n",
            "Epoch 74/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.9110 - accuracy: 0.7008 - val_loss: 2.1674 - val_accuracy: 0.4639\n",
            "Epoch 75/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.6209 - accuracy: 0.7766 - val_loss: 1.1580 - val_accuracy: 0.6267\n",
            "Epoch 76/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 0.3830 - accuracy: 0.8686 - val_loss: 1.0924 - val_accuracy: 0.6199\n",
            "Epoch 77/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.4784 - accuracy: 0.8351 - val_loss: 1.1572 - val_accuracy: 0.6053\n",
            "Epoch 78/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.5318 - accuracy: 0.8059 - val_loss: 0.9685 - val_accuracy: 0.6657\n",
            "Epoch 79/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.3027 - accuracy: 0.8888 - val_loss: 1.0500 - val_accuracy: 0.6404\n",
            "Epoch 80/100\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 0.2392 - accuracy: 0.9112 - val_loss: 1.4381 - val_accuracy: 0.6092\n",
            "Epoch 81/100\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 0.7505 - accuracy: 0.7613 - val_loss: 1.2019 - val_accuracy: 0.6345\n",
            "Epoch 82/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 0.3204 - accuracy: 0.8924 - val_loss: 1.1070 - val_accuracy: 0.6374\n",
            "Epoch 83/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.4535 - accuracy: 0.8270 - val_loss: 1.6935 - val_accuracy: 0.5166\n",
            "Epoch 84/100\n",
            "49/49 [==============================] - 13s 269ms/step - loss: 0.3744 - accuracy: 0.8592 - val_loss: 1.3856 - val_accuracy: 0.6082\n",
            "Epoch 85/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 0.2017 - accuracy: 0.9298 - val_loss: 1.3229 - val_accuracy: 0.6384\n",
            "Epoch 86/100\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 0.6440 - accuracy: 0.8185 - val_loss: 1.0095 - val_accuracy: 0.5780\n",
            "Epoch 87/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.4693 - accuracy: 0.8429 - val_loss: 1.1890 - val_accuracy: 0.5897\n",
            "Epoch 88/100\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 0.2446 - accuracy: 0.9138 - val_loss: 2.8698 - val_accuracy: 0.4727\n",
            "Epoch 89/100\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 0.3993 - accuracy: 0.8722 - val_loss: 1.2037 - val_accuracy: 0.6316\n",
            "Epoch 90/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.2832 - accuracy: 0.8979 - val_loss: 1.2473 - val_accuracy: 0.6277\n",
            "Epoch 91/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.2758 - accuracy: 0.9044 - val_loss: 1.3309 - val_accuracy: 0.5926\n",
            "Epoch 92/100\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 0.2566 - accuracy: 0.9073 - val_loss: 1.1080 - val_accuracy: 0.6433\n",
            "Epoch 93/100\n",
            "49/49 [==============================] - 13s 265ms/step - loss: 0.2997 - accuracy: 0.9011 - val_loss: 1.1850 - val_accuracy: 0.6238\n",
            "Epoch 94/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.4698 - accuracy: 0.8371 - val_loss: 1.0574 - val_accuracy: 0.6189\n",
            "Epoch 95/100\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 0.2142 - accuracy: 0.9298 - val_loss: 1.2894 - val_accuracy: 0.6628\n",
            "Epoch 96/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.2162 - accuracy: 0.9193 - val_loss: 9.8208 - val_accuracy: 0.3168\n",
            "Epoch 97/100\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 2.0306 - accuracy: 0.5018 - val_loss: 1.3319 - val_accuracy: 0.5253\n",
            "Epoch 98/100\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 0.6716 - accuracy: 0.7411 - val_loss: 0.8867 - val_accuracy: 0.6326\n",
            "Epoch 99/100\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 0.5043 - accuracy: 0.8039 - val_loss: 2.3284 - val_accuracy: 0.4922\n",
            "Epoch 100/100\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 0.5887 - accuracy: 0.7922 - val_loss: 2.9330 - val_accuracy: 0.5078\n"
          ]
        }
      ],
      "source": [
        "# Import VGG16\n",
        "vgg16_tl = VGG16(include_top=False, input_shape=(500, 500, 3))\n",
        "\n",
        "# Freezing VGG16 layers\n",
        "for layer in vgg16_tl.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Entry layer for instance population inputs\n",
        "integer_input = Input(shape=(1,), name='integer_input')\n",
        "\n",
        "# Flattening VGG16 output\n",
        "image_features = layers.Flatten()(vgg16_tl.layers[-1].output)\n",
        "\n",
        "# Merge VGG16 output with instance population\n",
        "merged_inputs = Concatenate()([image_features, integer_input])\n",
        "\n",
        "# Dense layers\n",
        "dense_1 = layers.Dense(256, activation='relu')(merged_inputs)\n",
        "dense_2 = layers.Dense(128, activation='relu')(dense_1)\n",
        "dense_3 = layers.Dense(64, activation='relu')(dense_2)\n",
        "dense_4 = layers.Dense(32, activation='relu')(dense_3)\n",
        "dense_f = layers.Dense(16, activation='relu')(dense_4)\n",
        "\n",
        "# Output layer \n",
        "output = layers.Dense(4, activation='softmax')(dense_f)\n",
        "\n",
        "# Define new model\n",
        "vgg16_merged_model = Model(inputs=[vgg16_tl.inputs, integer_input], outputs=output)\n",
        "\n",
        "# Model summary\n",
        "vgg16_merged_model.summary()\n",
        "\n",
        "# Compile model\n",
        "vgg16_merged_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping (Not working as expected)\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=5,  restore_best_weights=True)\n",
        "\n",
        "# Model fit\n",
        "vgg_train = vgg16_merged_model.fit([X_train_img_0, X_train_popn_0], y_train_onehot, validation_data=([X_val_img_0, X_val_popn_0], y_val_onehot), batch_size=64 ,epochs=100, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C. ResNet50"
      ],
      "metadata": {
        "id": "mVie5py4FXlI"
      },
      "id": "mVie5py4FXlI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import ResNet50\n",
        "resnet50_tl = ResNet50(include_top=False, input_shape=(500, 500, 3))\n",
        "\n",
        "# Freezing ResNet50 layers\n",
        "for layer in resnet50_tl.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Entry layer for instance population inputs\n",
        "integer_input = Input(shape=(1,), name='integer_input')\n",
        "\n",
        "# Flattening ResNet50 output\n",
        "image_features = layers.Flatten()(resnet50_tl.layers[-1].output)\n",
        "\n",
        "# Merge ResNet50 output with instance population\n",
        "merged_inputs = Concatenate()([image_features, integer_input])\n",
        "\n",
        "# Dense layers\n",
        "#dense_1 = layers.Dense(16384, activation='relu')(flat_1)\n",
        "#dense_2 = layers.Dense(2048, activation='relu')(flat_1)\n",
        "#dense_3 = layers.Dense(500, activation='relu')(flat_1)\n",
        "dense_4 = layers.Dense(256, activation='relu')(merged_inputs)\n",
        "dense_5 = layers.Dense(64, activation='relu')(dense_4)\n",
        "dense_f = layers.Dense(16, activation='relu')(dense_5)\n",
        "\n",
        "# Output layer  \n",
        "output = layers.Dense(4, activation='softmax')(dense_f)\n",
        "\n",
        "# Define new model\n",
        "resnet_merged_model = Model(inputs=[resnet50_tl.inputs, integer_input], outputs=output)\n",
        "\n",
        "# Model summary\n",
        "resnet_merged_model.summary()\n",
        "\n",
        "# Compile model\n",
        "resnet_merged_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping (Not working as expected)\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=5,  restore_best_weights=True)\n",
        "\n",
        "# Model fit\n",
        "resnet_train = resnet_merged_model.fit([X_train_img_0, X_train_popn_0], y_train_onehot, validation_data=([X_val_img_0, X_val_popn_0], y_val_onehot), batch_size=64 ,epochs=100, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0usIxJDHUyM",
        "outputId": "993287d5-eaae-4223-d172-24fcbfb879f8"
      },
      "id": "-0usIxJDHUyM",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 500, 500, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv1_pad (ZeroPadding2D)      (None, 506, 506, 3)  0           ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv1_conv (Conv2D)            (None, 250, 250, 64  9472        ['conv1_pad[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv1_bn (BatchNormalization)  (None, 250, 250, 64  256         ['conv1_conv[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv1_relu (Activation)        (None, 250, 250, 64  0           ['conv1_bn[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " pool1_pad (ZeroPadding2D)      (None, 252, 252, 64  0           ['conv1_relu[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " pool1_pool (MaxPooling2D)      (None, 125, 125, 64  0           ['pool1_pad[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2D)   (None, 125, 125, 64  4160        ['pool1_pool[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNormal  (None, 125, 125, 64  256        ['conv2_block1_1_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activatio  (None, 125, 125, 64  0          ['conv2_block1_1_bn[0][0]']      \n",
            " n)                             )                                                                 \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2D)   (None, 125, 125, 64  36928       ['conv2_block1_1_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2_block1_2_bn (BatchNormal  (None, 125, 125, 64  256        ['conv2_block1_2_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv2_block1_2_relu (Activatio  (None, 125, 125, 64  0          ['conv2_block1_2_bn[0][0]']      \n",
            " n)                             )                                                                 \n",
            "                                                                                                  \n",
            " conv2_block1_0_conv (Conv2D)   (None, 125, 125, 25  16640       ['pool1_pool[0][0]']             \n",
            "                                6)                                                                \n",
            "                                                                                                  \n",
            " conv2_block1_3_conv (Conv2D)   (None, 125, 125, 25  16640       ['conv2_block1_2_relu[0][0]']    \n",
            "                                6)                                                                \n",
            "                                                                                                  \n",
            " conv2_block1_0_bn (BatchNormal  (None, 125, 125, 25  1024       ['conv2_block1_0_conv[0][0]']    \n",
            " ization)                       6)                                                                \n",
            "                                                                                                  \n",
            " conv2_block1_3_bn (BatchNormal  (None, 125, 125, 25  1024       ['conv2_block1_3_conv[0][0]']    \n",
            " ization)                       6)                                                                \n",
            "                                                                                                  \n",
            " conv2_block1_add (Add)         (None, 125, 125, 25  0           ['conv2_block1_0_bn[0][0]',      \n",
            "                                6)                                'conv2_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block1_out (Activation)  (None, 125, 125, 25  0           ['conv2_block1_add[0][0]']       \n",
            "                                6)                                                                \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2D)   (None, 125, 125, 64  16448       ['conv2_block1_out[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNormal  (None, 125, 125, 64  256        ['conv2_block2_1_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activatio  (None, 125, 125, 64  0          ['conv2_block2_1_bn[0][0]']      \n",
            " n)                             )                                                                 \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2D)   (None, 125, 125, 64  36928       ['conv2_block2_1_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2_block2_2_bn (BatchNormal  (None, 125, 125, 64  256        ['conv2_block2_2_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv2_block2_2_relu (Activatio  (None, 125, 125, 64  0          ['conv2_block2_2_bn[0][0]']      \n",
            " n)                             )                                                                 \n",
            "                                                                                                  \n",
            " conv2_block2_3_conv (Conv2D)   (None, 125, 125, 25  16640       ['conv2_block2_2_relu[0][0]']    \n",
            "                                6)                                                                \n",
            "                                                                                                  \n",
            " conv2_block2_3_bn (BatchNormal  (None, 125, 125, 25  1024       ['conv2_block2_3_conv[0][0]']    \n",
            " ization)                       6)                                                                \n",
            "                                                                                                  \n",
            " conv2_block2_add (Add)         (None, 125, 125, 25  0           ['conv2_block1_out[0][0]',       \n",
            "                                6)                                'conv2_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block2_out (Activation)  (None, 125, 125, 25  0           ['conv2_block2_add[0][0]']       \n",
            "                                6)                                                                \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2D)   (None, 125, 125, 64  16448       ['conv2_block2_out[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNormal  (None, 125, 125, 64  256        ['conv2_block3_1_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activatio  (None, 125, 125, 64  0          ['conv2_block3_1_bn[0][0]']      \n",
            " n)                             )                                                                 \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2D)   (None, 125, 125, 64  36928       ['conv2_block3_1_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2_block3_2_bn (BatchNormal  (None, 125, 125, 64  256        ['conv2_block3_2_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv2_block3_2_relu (Activatio  (None, 125, 125, 64  0          ['conv2_block3_2_bn[0][0]']      \n",
            " n)                             )                                                                 \n",
            "                                                                                                  \n",
            " conv2_block3_3_conv (Conv2D)   (None, 125, 125, 25  16640       ['conv2_block3_2_relu[0][0]']    \n",
            "                                6)                                                                \n",
            "                                                                                                  \n",
            " conv2_block3_3_bn (BatchNormal  (None, 125, 125, 25  1024       ['conv2_block3_3_conv[0][0]']    \n",
            " ization)                       6)                                                                \n",
            "                                                                                                  \n",
            " conv2_block3_add (Add)         (None, 125, 125, 25  0           ['conv2_block2_out[0][0]',       \n",
            "                                6)                                'conv2_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block3_out (Activation)  (None, 125, 125, 25  0           ['conv2_block3_add[0][0]']       \n",
            "                                6)                                                                \n",
            "                                                                                                  \n",
            " conv3_block1_1_conv (Conv2D)   (None, 63, 63, 128)  32896       ['conv2_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_1_bn (BatchNormal  (None, 63, 63, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_1_relu (Activatio  (None, 63, 63, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_2_conv (Conv2D)   (None, 63, 63, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_2_bn (BatchNormal  (None, 63, 63, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_2_relu (Activatio  (None, 63, 63, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_0_conv (Conv2D)   (None, 63, 63, 512)  131584      ['conv2_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_3_conv (Conv2D)   (None, 63, 63, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_0_bn (BatchNormal  (None, 63, 63, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_3_bn (BatchNormal  (None, 63, 63, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_add (Add)         (None, 63, 63, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
            "                                                                  'conv3_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block1_out (Activation)  (None, 63, 63, 512)  0           ['conv3_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block2_1_conv (Conv2D)   (None, 63, 63, 128)  65664       ['conv3_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block2_1_bn (BatchNormal  (None, 63, 63, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_1_relu (Activatio  (None, 63, 63, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_2_conv (Conv2D)   (None, 63, 63, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_2_bn (BatchNormal  (None, 63, 63, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_2_relu (Activatio  (None, 63, 63, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_3_conv (Conv2D)   (None, 63, 63, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_3_bn (BatchNormal  (None, 63, 63, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_add (Add)         (None, 63, 63, 512)  0           ['conv3_block1_out[0][0]',       \n",
            "                                                                  'conv3_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block2_out (Activation)  (None, 63, 63, 512)  0           ['conv3_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block3_1_conv (Conv2D)   (None, 63, 63, 128)  65664       ['conv3_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block3_1_bn (BatchNormal  (None, 63, 63, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_1_relu (Activatio  (None, 63, 63, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_2_conv (Conv2D)   (None, 63, 63, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_2_bn (BatchNormal  (None, 63, 63, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_2_relu (Activatio  (None, 63, 63, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_3_conv (Conv2D)   (None, 63, 63, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_3_bn (BatchNormal  (None, 63, 63, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_add (Add)         (None, 63, 63, 512)  0           ['conv3_block2_out[0][0]',       \n",
            "                                                                  'conv3_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block3_out (Activation)  (None, 63, 63, 512)  0           ['conv3_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block4_1_conv (Conv2D)   (None, 63, 63, 128)  65664       ['conv3_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block4_1_bn (BatchNormal  (None, 63, 63, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_1_relu (Activatio  (None, 63, 63, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_2_conv (Conv2D)   (None, 63, 63, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_2_bn (BatchNormal  (None, 63, 63, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_2_relu (Activatio  (None, 63, 63, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_3_conv (Conv2D)   (None, 63, 63, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_3_bn (BatchNormal  (None, 63, 63, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_add (Add)         (None, 63, 63, 512)  0           ['conv3_block3_out[0][0]',       \n",
            "                                                                  'conv3_block4_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block4_out (Activation)  (None, 63, 63, 512)  0           ['conv3_block4_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_1_conv (Conv2D)   (None, 32, 32, 256)  131328      ['conv3_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_1_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_1_relu (Activatio  (None, 32, 32, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_2_conv (Conv2D)   (None, 32, 32, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block1_2_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_2_relu (Activatio  (None, 32, 32, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_0_conv (Conv2D)   (None, 32, 32, 1024  525312      ['conv3_block4_out[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_3_conv (Conv2D)   (None, 32, 32, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_0_bn (BatchNormal  (None, 32, 32, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_3_bn (BatchNormal  (None, 32, 32, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_add (Add)         (None, 32, 32, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
            "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block1_out (Activation)  (None, 32, 32, 1024  0           ['conv4_block1_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_1_conv (Conv2D)   (None, 32, 32, 256)  262400      ['conv4_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block2_1_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_1_relu (Activatio  (None, 32, 32, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_2_conv (Conv2D)   (None, 32, 32, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block2_2_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_2_relu (Activatio  (None, 32, 32, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_3_conv (Conv2D)   (None, 32, 32, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_3_bn (BatchNormal  (None, 32, 32, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_add (Add)         (None, 32, 32, 1024  0           ['conv4_block1_out[0][0]',       \n",
            "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block2_out (Activation)  (None, 32, 32, 1024  0           ['conv4_block2_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_1_conv (Conv2D)   (None, 32, 32, 256)  262400      ['conv4_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block3_1_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_1_relu (Activatio  (None, 32, 32, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_2_conv (Conv2D)   (None, 32, 32, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block3_2_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_2_relu (Activatio  (None, 32, 32, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_3_conv (Conv2D)   (None, 32, 32, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_3_bn (BatchNormal  (None, 32, 32, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_add (Add)         (None, 32, 32, 1024  0           ['conv4_block2_out[0][0]',       \n",
            "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block3_out (Activation)  (None, 32, 32, 1024  0           ['conv4_block3_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_1_conv (Conv2D)   (None, 32, 32, 256)  262400      ['conv4_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block4_1_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_1_relu (Activatio  (None, 32, 32, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_2_conv (Conv2D)   (None, 32, 32, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block4_2_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_2_relu (Activatio  (None, 32, 32, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_3_conv (Conv2D)   (None, 32, 32, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_3_bn (BatchNormal  (None, 32, 32, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_add (Add)         (None, 32, 32, 1024  0           ['conv4_block3_out[0][0]',       \n",
            "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block4_out (Activation)  (None, 32, 32, 1024  0           ['conv4_block4_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_1_conv (Conv2D)   (None, 32, 32, 256)  262400      ['conv4_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block5_1_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_1_relu (Activatio  (None, 32, 32, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_2_conv (Conv2D)   (None, 32, 32, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block5_2_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_2_relu (Activatio  (None, 32, 32, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_3_conv (Conv2D)   (None, 32, 32, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_3_bn (BatchNormal  (None, 32, 32, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_add (Add)         (None, 32, 32, 1024  0           ['conv4_block4_out[0][0]',       \n",
            "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block5_out (Activation)  (None, 32, 32, 1024  0           ['conv4_block5_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_1_conv (Conv2D)   (None, 32, 32, 256)  262400      ['conv4_block5_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block6_1_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_1_relu (Activatio  (None, 32, 32, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_2_conv (Conv2D)   (None, 32, 32, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block6_2_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_2_relu (Activatio  (None, 32, 32, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_3_conv (Conv2D)   (None, 32, 32, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_3_bn (BatchNormal  (None, 32, 32, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_add (Add)         (None, 32, 32, 1024  0           ['conv4_block5_out[0][0]',       \n",
            "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block6_out (Activation)  (None, 32, 32, 1024  0           ['conv4_block6_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv5_block1_1_conv (Conv2D)   (None, 16, 16, 512)  524800      ['conv4_block6_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block1_1_bn (BatchNormal  (None, 16, 16, 512)  2048       ['conv5_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_1_relu (Activatio  (None, 16, 16, 512)  0          ['conv5_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_2_conv (Conv2D)   (None, 16, 16, 512)  2359808     ['conv5_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block1_2_bn (BatchNormal  (None, 16, 16, 512)  2048       ['conv5_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_2_relu (Activatio  (None, 16, 16, 512)  0          ['conv5_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_0_conv (Conv2D)   (None, 16, 16, 2048  2099200     ['conv4_block6_out[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv5_block1_3_conv (Conv2D)   (None, 16, 16, 2048  1050624     ['conv5_block1_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv5_block1_0_bn (BatchNormal  (None, 16, 16, 2048  8192       ['conv5_block1_0_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv5_block1_3_bn (BatchNormal  (None, 16, 16, 2048  8192       ['conv5_block1_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv5_block1_add (Add)         (None, 16, 16, 2048  0           ['conv5_block1_0_bn[0][0]',      \n",
            "                                )                                 'conv5_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block1_out (Activation)  (None, 16, 16, 2048  0           ['conv5_block1_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv5_block2_1_conv (Conv2D)   (None, 16, 16, 512)  1049088     ['conv5_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block2_1_bn (BatchNormal  (None, 16, 16, 512)  2048       ['conv5_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_1_relu (Activatio  (None, 16, 16, 512)  0          ['conv5_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_2_conv (Conv2D)   (None, 16, 16, 512)  2359808     ['conv5_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block2_2_bn (BatchNormal  (None, 16, 16, 512)  2048       ['conv5_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_2_relu (Activatio  (None, 16, 16, 512)  0          ['conv5_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_3_conv (Conv2D)   (None, 16, 16, 2048  1050624     ['conv5_block2_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv5_block2_3_bn (BatchNormal  (None, 16, 16, 2048  8192       ['conv5_block2_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv5_block2_add (Add)         (None, 16, 16, 2048  0           ['conv5_block1_out[0][0]',       \n",
            "                                )                                 'conv5_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block2_out (Activation)  (None, 16, 16, 2048  0           ['conv5_block2_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv5_block3_1_conv (Conv2D)   (None, 16, 16, 512)  1049088     ['conv5_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block3_1_bn (BatchNormal  (None, 16, 16, 512)  2048       ['conv5_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_1_relu (Activatio  (None, 16, 16, 512)  0          ['conv5_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_2_conv (Conv2D)   (None, 16, 16, 512)  2359808     ['conv5_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block3_2_bn (BatchNormal  (None, 16, 16, 512)  2048       ['conv5_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_2_relu (Activatio  (None, 16, 16, 512)  0          ['conv5_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_3_conv (Conv2D)   (None, 16, 16, 2048  1050624     ['conv5_block3_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv5_block3_3_bn (BatchNormal  (None, 16, 16, 2048  8192       ['conv5_block3_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv5_block3_add (Add)         (None, 16, 16, 2048  0           ['conv5_block2_out[0][0]',       \n",
            "                                )                                 'conv5_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block3_out (Activation)  (None, 16, 16, 2048  0           ['conv5_block3_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)            (None, 524288)       0           ['conv5_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " integer_input (InputLayer)     [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 524289)       0           ['flatten_3[0][0]',              \n",
            "                                                                  'integer_input[0][0]']          \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 256)          134218240   ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 64)           16448       ['dense_13[0][0]']               \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 16)           1040        ['dense_14[0][0]']               \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 4)            68          ['dense_15[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 157,823,508\n",
            "Trainable params: 134,235,796\n",
            "Non-trainable params: 23,587,712\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "49/49 [==============================] - 22s 296ms/step - loss: 82.5195 - accuracy: 0.3070 - val_loss: 34.7838 - val_accuracy: 0.1530\n",
            "Epoch 2/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 13.2936 - accuracy: 0.3971 - val_loss: 9.2114 - val_accuracy: 0.2320\n",
            "Epoch 3/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 15.4130 - accuracy: 0.3486 - val_loss: 25.8792 - val_accuracy: 0.3333\n",
            "Epoch 4/100\n",
            "49/49 [==============================] - 10s 195ms/step - loss: 13.0115 - accuracy: 0.3925 - val_loss: 21.8418 - val_accuracy: 0.3801\n",
            "Epoch 5/100\n",
            "49/49 [==============================] - 10s 195ms/step - loss: 13.4608 - accuracy: 0.3958 - val_loss: 9.3852 - val_accuracy: 0.4571\n",
            "Epoch 6/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 7.7709 - accuracy: 0.4224 - val_loss: 13.4545 - val_accuracy: 0.3743\n",
            "Epoch 7/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 13.1275 - accuracy: 0.3889 - val_loss: 22.7075 - val_accuracy: 0.2943\n",
            "Epoch 8/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 8.5991 - accuracy: 0.4302 - val_loss: 23.2536 - val_accuracy: 0.2641\n",
            "Epoch 9/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 6.4365 - accuracy: 0.4517 - val_loss: 21.8066 - val_accuracy: 0.2641\n",
            "Epoch 10/100\n",
            "49/49 [==============================] - 10s 199ms/step - loss: 8.5571 - accuracy: 0.3967 - val_loss: 7.1733 - val_accuracy: 0.3587\n",
            "Epoch 11/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 7.8104 - accuracy: 0.4153 - val_loss: 16.5540 - val_accuracy: 0.3470\n",
            "Epoch 12/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 6.4128 - accuracy: 0.4237 - val_loss: 4.0279 - val_accuracy: 0.3957\n",
            "Epoch 13/100\n",
            "49/49 [==============================] - 10s 198ms/step - loss: 4.2099 - accuracy: 0.4455 - val_loss: 6.2444 - val_accuracy: 0.4103\n",
            "Epoch 14/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 6.5389 - accuracy: 0.4085 - val_loss: 22.0873 - val_accuracy: 0.3168\n",
            "Epoch 15/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 9.7753 - accuracy: 0.3811 - val_loss: 4.0771 - val_accuracy: 0.4659\n",
            "Epoch 16/100\n",
            "49/49 [==============================] - 10s 198ms/step - loss: 3.0927 - accuracy: 0.4699 - val_loss: 17.5996 - val_accuracy: 0.2641\n",
            "Epoch 17/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 9.9418 - accuracy: 0.3694 - val_loss: 2.4515 - val_accuracy: 0.4620\n",
            "Epoch 18/100\n",
            "49/49 [==============================] - 10s 198ms/step - loss: 2.0176 - accuracy: 0.4345 - val_loss: 2.9775 - val_accuracy: 0.3626\n",
            "Epoch 19/100\n",
            "49/49 [==============================] - 10s 198ms/step - loss: 2.0016 - accuracy: 0.4459 - val_loss: 1.2605 - val_accuracy: 0.5097\n",
            "Epoch 20/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.6657 - accuracy: 0.4589 - val_loss: 1.6507 - val_accuracy: 0.4064\n",
            "Epoch 21/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.7366 - accuracy: 0.4400 - val_loss: 3.3476 - val_accuracy: 0.2973\n",
            "Epoch 22/100\n",
            "49/49 [==============================] - 10s 198ms/step - loss: 1.5528 - accuracy: 0.4787 - val_loss: 7.7590 - val_accuracy: 0.2914\n",
            "Epoch 23/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 2.6353 - accuracy: 0.4315 - val_loss: 2.1281 - val_accuracy: 0.4581\n",
            "Epoch 24/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.5211 - accuracy: 0.4644 - val_loss: 1.2900 - val_accuracy: 0.4776\n",
            "Epoch 25/100\n",
            "49/49 [==============================] - 10s 195ms/step - loss: 1.2586 - accuracy: 0.4862 - val_loss: 1.6344 - val_accuracy: 0.4893\n",
            "Epoch 26/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.2579 - accuracy: 0.4946 - val_loss: 2.4976 - val_accuracy: 0.3119\n",
            "Epoch 27/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.8786 - accuracy: 0.4410 - val_loss: 2.0220 - val_accuracy: 0.3060\n",
            "Epoch 28/100\n",
            "49/49 [==============================] - 10s 198ms/step - loss: 1.1669 - accuracy: 0.5089 - val_loss: 1.2403 - val_accuracy: 0.4864\n",
            "Epoch 29/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.1216 - accuracy: 0.5197 - val_loss: 3.9409 - val_accuracy: 0.3197\n",
            "Epoch 30/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.4308 - accuracy: 0.4637 - val_loss: 1.4761 - val_accuracy: 0.4522\n",
            "Epoch 31/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.2507 - accuracy: 0.5060 - val_loss: 1.9459 - val_accuracy: 0.3791\n",
            "Epoch 32/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.4557 - accuracy: 0.4543 - val_loss: 1.7414 - val_accuracy: 0.4269\n",
            "Epoch 33/100\n",
            "49/49 [==============================] - 10s 198ms/step - loss: 1.2112 - accuracy: 0.5239 - val_loss: 1.1163 - val_accuracy: 0.5205\n",
            "Epoch 34/100\n",
            "49/49 [==============================] - 10s 198ms/step - loss: 1.1434 - accuracy: 0.5154 - val_loss: 1.4443 - val_accuracy: 0.4201\n",
            "Epoch 35/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.1422 - accuracy: 0.5317 - val_loss: 1.8372 - val_accuracy: 0.4415\n",
            "Epoch 36/100\n",
            "49/49 [==============================] - 10s 198ms/step - loss: 1.0804 - accuracy: 0.5493 - val_loss: 1.0738 - val_accuracy: 0.5068\n",
            "Epoch 37/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.2781 - accuracy: 0.4800 - val_loss: 1.1430 - val_accuracy: 0.5253\n",
            "Epoch 38/100\n",
            "49/49 [==============================] - 10s 198ms/step - loss: 1.5257 - accuracy: 0.4663 - val_loss: 2.7331 - val_accuracy: 0.3665\n",
            "Epoch 39/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.2936 - accuracy: 0.5112 - val_loss: 1.2451 - val_accuracy: 0.4883\n",
            "Epoch 40/100\n",
            "49/49 [==============================] - 10s 198ms/step - loss: 1.0543 - accuracy: 0.5541 - val_loss: 1.3945 - val_accuracy: 0.4698\n",
            "Epoch 41/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.1106 - accuracy: 0.5376 - val_loss: 1.9497 - val_accuracy: 0.3499\n",
            "Epoch 42/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.0693 - accuracy: 0.5571 - val_loss: 2.4338 - val_accuracy: 0.3996\n",
            "Epoch 43/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.6763 - accuracy: 0.4494 - val_loss: 3.0764 - val_accuracy: 0.2963\n",
            "Epoch 44/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.3357 - accuracy: 0.4976 - val_loss: 1.7911 - val_accuracy: 0.4766\n",
            "Epoch 45/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.1053 - accuracy: 0.5545 - val_loss: 1.1774 - val_accuracy: 0.4834\n",
            "Epoch 46/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.0064 - accuracy: 0.5626 - val_loss: 1.0227 - val_accuracy: 0.5702\n",
            "Epoch 47/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.0257 - accuracy: 0.5613 - val_loss: 1.5268 - val_accuracy: 0.4025\n",
            "Epoch 48/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.1306 - accuracy: 0.5158 - val_loss: 1.7511 - val_accuracy: 0.4786\n",
            "Epoch 49/100\n",
            "49/49 [==============================] - 10s 198ms/step - loss: 1.1159 - accuracy: 0.5506 - val_loss: 1.8239 - val_accuracy: 0.4103\n",
            "Epoch 50/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.0417 - accuracy: 0.5655 - val_loss: 2.0558 - val_accuracy: 0.3470\n",
            "Epoch 51/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.0822 - accuracy: 0.5470 - val_loss: 1.9747 - val_accuracy: 0.4503\n",
            "Epoch 52/100\n",
            "49/49 [==============================] - 10s 198ms/step - loss: 1.4390 - accuracy: 0.5037 - val_loss: 1.2575 - val_accuracy: 0.4620\n",
            "Epoch 53/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 0.9769 - accuracy: 0.5753 - val_loss: 0.9946 - val_accuracy: 0.5809\n",
            "Epoch 54/100\n",
            "49/49 [==============================] - 10s 198ms/step - loss: 1.0037 - accuracy: 0.5691 - val_loss: 0.9933 - val_accuracy: 0.5770\n",
            "Epoch 55/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.0161 - accuracy: 0.5590 - val_loss: 3.5469 - val_accuracy: 0.1696\n",
            "Epoch 56/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.3655 - accuracy: 0.4924 - val_loss: 2.2836 - val_accuracy: 0.3655\n",
            "Epoch 57/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.4024 - accuracy: 0.4075 - val_loss: 1.2931 - val_accuracy: 0.3977\n",
            "Epoch 58/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.3582 - accuracy: 0.3486 - val_loss: 1.3735 - val_accuracy: 0.3090\n",
            "Epoch 59/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.3700 - accuracy: 0.3080 - val_loss: 1.3651 - val_accuracy: 0.3090\n",
            "Epoch 60/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.3622 - accuracy: 0.3080 - val_loss: 1.3582 - val_accuracy: 0.3090\n",
            "Epoch 61/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.3561 - accuracy: 0.3080 - val_loss: 1.3526 - val_accuracy: 0.3090\n",
            "Epoch 62/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.3510 - accuracy: 0.3080 - val_loss: 1.3480 - val_accuracy: 0.3090\n",
            "Epoch 63/100\n",
            "49/49 [==============================] - 10s 195ms/step - loss: 1.3467 - accuracy: 0.3080 - val_loss: 1.3441 - val_accuracy: 0.3090\n",
            "Epoch 64/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.3429 - accuracy: 0.3080 - val_loss: 1.3405 - val_accuracy: 0.3090\n",
            "Epoch 65/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.3397 - accuracy: 0.3080 - val_loss: 1.3375 - val_accuracy: 0.3090\n",
            "Epoch 66/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.3371 - accuracy: 0.3080 - val_loss: 1.3353 - val_accuracy: 0.3090\n",
            "Epoch 67/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.3349 - accuracy: 0.3080 - val_loss: 1.3331 - val_accuracy: 0.3090\n",
            "Epoch 68/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.3330 - accuracy: 0.3080 - val_loss: 1.3314 - val_accuracy: 0.3090\n",
            "Epoch 69/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.3314 - accuracy: 0.3080 - val_loss: 1.3299 - val_accuracy: 0.3090\n",
            "Epoch 70/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.3300 - accuracy: 0.3080 - val_loss: 1.3287 - val_accuracy: 0.3090\n",
            "Epoch 71/100\n",
            "49/49 [==============================] - 10s 198ms/step - loss: 1.3291 - accuracy: 0.3080 - val_loss: 1.3278 - val_accuracy: 0.3090\n",
            "Epoch 72/100\n",
            "49/49 [==============================] - 10s 195ms/step - loss: 1.3281 - accuracy: 0.3080 - val_loss: 1.3269 - val_accuracy: 0.3090\n",
            "Epoch 73/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.3272 - accuracy: 0.3080 - val_loss: 1.3261 - val_accuracy: 0.3090\n",
            "Epoch 74/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.3264 - accuracy: 0.3080 - val_loss: 1.3254 - val_accuracy: 0.3090\n",
            "Epoch 75/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.3258 - accuracy: 0.3080 - val_loss: 1.3248 - val_accuracy: 0.3090\n",
            "Epoch 76/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.3252 - accuracy: 0.3080 - val_loss: 1.3243 - val_accuracy: 0.3090\n",
            "Epoch 77/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.3247 - accuracy: 0.3161 - val_loss: 1.3238 - val_accuracy: 0.3177\n",
            "Epoch 78/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.3243 - accuracy: 0.3174 - val_loss: 1.3235 - val_accuracy: 0.3177\n",
            "Epoch 79/100\n",
            "49/49 [==============================] - 10s 198ms/step - loss: 1.3240 - accuracy: 0.3174 - val_loss: 1.3232 - val_accuracy: 0.3177\n",
            "Epoch 80/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.3238 - accuracy: 0.3174 - val_loss: 1.3229 - val_accuracy: 0.3177\n",
            "Epoch 81/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.3235 - accuracy: 0.3174 - val_loss: 1.3227 - val_accuracy: 0.3177\n",
            "Epoch 82/100\n",
            "49/49 [==============================] - 10s 195ms/step - loss: 1.3234 - accuracy: 0.3174 - val_loss: 1.3226 - val_accuracy: 0.3177\n",
            "Epoch 83/100\n",
            "49/49 [==============================] - 10s 199ms/step - loss: 1.3233 - accuracy: 0.3174 - val_loss: 1.3224 - val_accuracy: 0.3177\n",
            "Epoch 84/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.3232 - accuracy: 0.3174 - val_loss: 1.3224 - val_accuracy: 0.3177\n",
            "Epoch 85/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.3231 - accuracy: 0.3174 - val_loss: 1.3223 - val_accuracy: 0.3177\n",
            "Epoch 86/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.3230 - accuracy: 0.3174 - val_loss: 1.3222 - val_accuracy: 0.3177\n",
            "Epoch 87/100\n",
            "49/49 [==============================] - 10s 195ms/step - loss: 1.3229 - accuracy: 0.3174 - val_loss: 1.3222 - val_accuracy: 0.3177\n",
            "Epoch 88/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.3228 - accuracy: 0.3174 - val_loss: 1.3221 - val_accuracy: 0.3177\n",
            "Epoch 89/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.3228 - accuracy: 0.3174 - val_loss: 1.3220 - val_accuracy: 0.3177\n",
            "Epoch 90/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.3228 - accuracy: 0.3174 - val_loss: 1.3220 - val_accuracy: 0.3177\n",
            "Epoch 91/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.3227 - accuracy: 0.3174 - val_loss: 1.3219 - val_accuracy: 0.3177\n",
            "Epoch 92/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.3227 - accuracy: 0.3174 - val_loss: 1.3219 - val_accuracy: 0.3177\n",
            "Epoch 93/100\n",
            "49/49 [==============================] - 10s 198ms/step - loss: 1.3227 - accuracy: 0.3174 - val_loss: 1.3219 - val_accuracy: 0.3177\n",
            "Epoch 94/100\n",
            "49/49 [==============================] - 10s 196ms/step - loss: 1.3226 - accuracy: 0.3174 - val_loss: 1.3219 - val_accuracy: 0.3177\n",
            "Epoch 95/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.3226 - accuracy: 0.3174 - val_loss: 1.3218 - val_accuracy: 0.3177\n",
            "Epoch 96/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.3225 - accuracy: 0.3174 - val_loss: 1.3218 - val_accuracy: 0.3177\n",
            "Epoch 97/100\n",
            "49/49 [==============================] - 10s 195ms/step - loss: 1.3225 - accuracy: 0.3174 - val_loss: 1.3218 - val_accuracy: 0.3177\n",
            "Epoch 98/100\n",
            "49/49 [==============================] - 10s 198ms/step - loss: 1.3225 - accuracy: 0.3174 - val_loss: 1.3218 - val_accuracy: 0.3177\n",
            "Epoch 99/100\n",
            "49/49 [==============================] - 10s 197ms/step - loss: 1.3225 - accuracy: 0.3174 - val_loss: 1.3218 - val_accuracy: 0.3177\n",
            "Epoch 100/100\n",
            "49/49 [==============================] - 10s 198ms/step - loss: 1.3225 - accuracy: 0.3174 - val_loss: 1.3218 - val_accuracy: 0.3177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### D. Alexnet"
      ],
      "metadata": {
        "id": "Nakc3QmARcLU"
      },
      "id": "Nakc3QmARcLU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Relevant information on image preprocessing for AlexNet that may be worth considering -> https://pytorch.org/hub/pytorch_vision_alexnet/\n",
        "\n",
        "# Convirtiendo las features ya normalizadas (/255) a pytorch tensor \n",
        "X_train = torch.tensor(X_train)\n",
        "X_val = torch.tensor(X_val)\n",
        "X_test = torch.tensor(X_test)\n",
        "\n",
        "# Conviertiendo los targets (onehot) a tensor\n",
        "y_train_onehot = torch.tensor(y_train_onehot)\n",
        "y_val_onehot = torch.tensor(y_val_onehot)\n",
        "y_test_onehot = torch.tensor(y_test_onehot)\n",
        "\n",
        "# Zip de los objetos anteriores para que el dataloader se comporte de la manera deseada\n",
        "# Al zip se le añade .permute para que la configuración de las dimensiones de los tensores \n",
        "# de entrada coincidan con las que espera el modelo.\n",
        "Xy_train = list(zip(X_train.permute(0, 3, 1, 2), y_train_onehot))\n",
        "Xy_val = list(zip(X_val.permute(0, 3, 1, 2), y_val_onehot))\n",
        "Xy_test = list(zip(X_test.permute(0, 3, 1, 2), y_test_onehot))\n",
        "\n",
        "# crea los dataloaders para cada conjunto\n",
        "train_dataloader = DataLoader(Xy_train, batch_size=64, shuffle=True)\n",
        "val_dataloader = DataLoader(Xy_val, batch_size=64, shuffle=False)\n",
        "test_dataloader = DataLoader(Xy_test, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# Preparar los indices (train / val) -> From RNN task notebook\n",
        "\n",
        "count = 0\n",
        "for batch, (x, y) in enumerate(train_dataloader):\n",
        "    count += (len(x*1))\n",
        "    \n",
        "train_indices_A = count\n",
        "\n",
        "\n",
        "count = 0    \n",
        "for batch, (x, y) in enumerate(val_dataloader):\n",
        "    count += (len(x*1))\n",
        "    \n",
        "val_indices_A = count\n",
        "\n",
        "count = 0    \n",
        "for batch, (x, y) in enumerate(test_dataloader):\n",
        "    count += (len(x*1))\n",
        "    \n",
        "val_test_A = count\n"
      ],
      "metadata": {
        "id": "NCswy0GyVpN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9708361-a178-49dc-a219-9381b6415d3a"
      },
      "id": "NCswy0GyVpN8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-74724c07d5c2>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_train = torch.tensor(X_train)\n",
            "<ipython-input-51-74724c07d5c2>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_val = torch.tensor(X_val)\n",
            "<ipython-input-51-74724c07d5c2>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_test = torch.tensor(X_test)\n",
            "<ipython-input-51-74724c07d5c2>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_train_onehot = torch.tensor(y_train_onehot)\n",
            "<ipython-input-51-74724c07d5c2>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_val_onehot = torch.tensor(y_val_onehot)\n",
            "<ipython-input-51-74724c07d5c2>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_test_onehot = torch.tensor(y_test_onehot)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing pre-trained AlexNet (Excepto esto todo es de GPT4)\n",
        "AlexNet = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)\n",
        "\n",
        "# Freezing pre-trained layers\n",
        "for param in AlexNet.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Eliminar la última capa para obtener las características de la imagen\n",
        "alexnet_features = nn.Sequential(*list(AlexNet.children())[:-1])\n",
        "\n",
        "# Crear una capa de entrada para el número entero y la capa de concatenación\n",
        "class ConcatenateInteger(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConcatenateInteger, self).__init__()\n",
        "    \n",
        "    def forward(self, image_features, integer_input):\n",
        "        integer_input = integer_input.view(-1, 1)\n",
        "        return torch.cat((image_features, integer_input), dim=1)\n",
        "\n",
        "concatenate_integer = ConcatenateInteger()\n",
        "\n",
        "# Crear la red neuronal compuesta por capas densas para la clasificación\n",
        "classifier = nn.Sequential(\n",
        "    nn.Linear(256 * 6 * 6 + 1, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 4),\n",
        ")\n",
        "\n",
        "# Crear la red completa\n",
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self, alexnet_features, concatenate_integer, classifier):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.alexnet_features = alexnet_features\n",
        "        self.concatenate_integer = concatenate_integer\n",
        "        self.classifier = classifier\n",
        "    \n",
        "    def forward(self, image, integer_input):\n",
        "        image_features = self.alexnet_features(image)\n",
        "        image_features = image_features.view(image_features.size(0), -1)\n",
        "        combined_features = self.concatenate_integer(image_features, integer_input)\n",
        "        return self.classifier(combined_features)\n",
        "\n",
        "model = CombinedModel(alexnet_features, concatenate_integer, classifier)\n",
        "model = model.double()\n",
        "\n",
        "# Compilar y entrenar el modelo\n",
        "# Asegúrate de tener tus datos en las variables X_images, X_integers y y_labels\n",
        "# X_images debe ser un tensor de PyTorch con las imágenes de tamaño (500, 500, 3)\n",
        "# X_integers debe ser un tensor de PyTorch con los números enteros\n",
        "# y_labels debe ser un tensor de PyTorch con las etiquetas de clasificación\n",
        "\n",
        "# Aquí tienes un ejemplo básico de cómo entrenar el modelo. \n",
        "# Puedes ajustar el código de entrenamiento según tus necesidades, \n",
        "# incluyendo el uso de conjuntos de datos personalizados, la división de entrenamiento/validación, etc.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    output = model(X_train_img_0, X_train_popn_0)\n",
        "    loss = criterion(output, y_train_onehot)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    print(f'Epoch: {epoch}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMRy0hTARavs",
        "outputId": "2c5e06f6-f532-43e6-f989-559f9a024118"
      },
      "id": "VMRy0hTARavs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_img_0 = torch.tensor(X_train_img_0).permute(0, 3, 1, 2)\n",
        "X_train_popn_0 = torch.tensor(X_train_popn_0)\n",
        "y_train_onehot = torch.tensor(y_train_onehot)"
      ],
      "metadata": {
        "id": "p7wpubIvmlhR"
      },
      "id": "p7wpubIvmlhR",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Congelando los parámetros de la primera mitad del modelo (AlexNet_model.features)\n",
        "# La otra mitad permanece descongelada (AlexNet_model.classifier)\n",
        "\n",
        "for param in AlexNet_model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model = AlexNet_model\n",
        "\n",
        "# verifica si una capa está congelada o descongelada\n",
        "for name, param in model.named_parameters():\n",
        "    if not param.requires_grad:\n",
        "        print(f'Capa {name} está congelada')\n",
        "    else:\n",
        "        print(f'Capa {name} está descongelada')\n",
        "\n",
        "# Include correct number of target classes (Originalmente hay mil incluidas)\n",
        "\n",
        "AlexNet_model.classifier[6] = nn.Linear(1024, 4)\n",
        "\n",
        "# Modify classifier 4 (En principio esto es opcional)\n",
        "\n",
        "AlexNet_model.classifier[4] = nn.Linear(4096,1024)\n",
        "\n",
        "# Resumen del modelo\n",
        "\n",
        "AlexNet_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k_wF4Nbezg6",
        "outputId": "4f983502-b99c-405a-fa9e-1f4997fed2f7"
      },
      "id": "7k_wF4Nbezg6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Capa features.0.weight está congelada\n",
            "Capa features.0.bias está congelada\n",
            "Capa features.3.weight está congelada\n",
            "Capa features.3.bias está congelada\n",
            "Capa features.6.weight está congelada\n",
            "Capa features.6.bias está congelada\n",
            "Capa features.8.weight está congelada\n",
            "Capa features.8.bias está congelada\n",
            "Capa features.10.weight está congelada\n",
            "Capa features.10.bias está congelada\n",
            "Capa classifier.1.weight está descongelada\n",
            "Capa classifier.1.bias está descongelada\n",
            "Capa classifier.4.weight está descongelada\n",
            "Capa classifier.4.bias está descongelada\n",
            "Capa classifier.6.weight está descongelada\n",
            "Capa classifier.6.bias está descongelada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping diseñado por narendra. No implementado por un problema de dependecias relativo a \"copy.deepcopy()\"\n",
        "\n",
        "def nn_converged_B(epoch: int, stop_after_epochs: int, validation_loss: torch.Tensor, model: torch.nn.Module) -> bool:\n",
        "    converged = False\n",
        "    # (Re)-start the epoch count with the first epoch or any improvement.\n",
        "    if epoch == 0 or validation_loss < model.best_validation_loss:\n",
        "        model.best_validation_loss = validation_loss\n",
        "        model.epochs_since_last_improvement = 0\n",
        "        model.best_model = copy.deepcopy(model.state_dict())\n",
        "    else:\n",
        "        model.epochs_since_last_improvement += 1\n",
        "\n",
        "    # If no validation improvement over many epochs, stop training.\n",
        "    if model.epochs_since_last_improvement > stop_after_epochs - 1:\n",
        "        model.load_state_dict(model.best_model)\n",
        "        converged = True\n",
        "    return converged"
      ],
      "metadata": {
        "id": "7xBUo4hkNca9"
      },
      "id": "7xBUo4hkNca9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = \"cuda\"\n",
        "model = AlexNet_model.to(device)\n",
        "\n",
        "#Define loss + optimizer\n",
        "criterion = nn.CrossEntropyLoss() # El mismo que en los otros dos modelos\n",
        "optimizer = optim.SGD(AlexNet_model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "phUzd1bbjogE"
      },
      "id": "phUzd1bbjogE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(\n",
        "        train_loader: DataLoader,\n",
        "        val_loader: DataLoader,\n",
        "        train_indices: torch.Tensor,\n",
        "        val_indices: torch.Tensor,\n",
        "        num_epochs: int = 100,\n",
        "        print_per_num_epochs: int = 5,\n",
        "        convergence_num_epochs: int = 15,\n",
        "    ) -> None:\n",
        "# Run the training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"epoch {epoch}\")\n",
        "        # Set the model in training mode for gradient evaluation\n",
        "        model.train()\n",
        "        # Set current loss value\n",
        "        train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        # Iterate over the DataLoader for training data\n",
        "        for batch, (inputs_an, targets) in enumerate(train_loader):\n",
        "            inputs_an = inputs_an.to(\"cuda\")\n",
        "            targets = targets.to(\"cuda\")\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Perform forward pass\n",
        "            out = model(inputs_an)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(out, targets).mean()\n",
        "\n",
        "            # Perform backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Perform optimization\n",
        "            optimizer.step()\n",
        "\n",
        "            # Add to the total of the training loss\n",
        "            train_loss += loss.item() * len(inputs_an)\n",
        "\n",
        "        # Once all training batches have been run, get the mean training loss\n",
        "        train_loss /= train_indices\n",
        "\n",
        "        # Set the model in evaluation mode so that gradients are not evaluated\n",
        "        model.eval()\n",
        "\n",
        "        val_loss = 0.0\n",
        "        correct_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, (inputs_an, targets) in enumerate(val_loader):\n",
        "                inputs_an = inputs_an.to(\"cuda\")\n",
        "                targets = targets.to(\"cuda\")\n",
        "                \n",
        "                # Formulate predictions\n",
        "                preds = model(inputs_an)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = criterion(preds, targets).mean()\n",
        "                val_loss += loss.item() * len(inputs_an)\n",
        "\n",
        "            # get mean validation loss\n",
        "            val_loss /= val_indices\n",
        "    \n",
        "\n",
        "        if epoch % print_per_num_epochs == 0: \n",
        "            print(f\"Train Loss after epoch: {epoch}: {train_loss}\") \n",
        "            print(f\"Validation loss after epoch: {epoch}: {val_loss}\") \n",
        "             \n",
        "\n",
        "        # EARLY STOPPING con nn_converged - CEGADO\n",
        "        #if nn_converged_B(epoch, convergence_num_epochs, val_loss, model): \n",
        "        #    print(f\"Stopping after epoch {epoch} as validation loss was not improving further\") \n",
        "        #   break "
      ],
      "metadata": {
        "id": "xtvL1x6-Rbt8"
      },
      "id": "xtvL1x6-Rbt8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit(train_loader = train_dataloader, val_loader = val_dataloader, train_indices = train_indices_A, val_indices = val_indices_A)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36oUUv-tFI5B",
        "outputId": "97a143db-649c-4474-c00d-0a84779a6b51"
      },
      "id": "36oUUv-tFI5B",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0\n",
            "Train Loss after epoch: 0: 0.8536575057351492\n",
            "Validation loss after epoch: 0: 0.8245534124197783\n",
            "epoch 1\n",
            "epoch 2\n",
            "epoch 3\n",
            "epoch 4\n",
            "epoch 5\n",
            "Train Loss after epoch: 5: 0.8222573174984474\n",
            "Validation loss after epoch: 5: 0.7998132866028457\n",
            "epoch 6\n",
            "epoch 7\n",
            "epoch 8\n",
            "epoch 9\n",
            "epoch 10\n",
            "Train Loss after epoch: 10: 0.8050134473215274\n",
            "Validation loss after epoch: 10: 0.7877727531434034\n",
            "epoch 11\n",
            "epoch 12\n",
            "epoch 13\n",
            "epoch 14\n",
            "epoch 15\n",
            "Train Loss after epoch: 15: 0.7646209873610396\n",
            "Validation loss after epoch: 15: 0.7676318558277907\n",
            "epoch 16\n",
            "epoch 17\n",
            "epoch 18\n",
            "epoch 19\n",
            "epoch 20\n",
            "Train Loss after epoch: 20: 0.7608069023271886\n",
            "Validation loss after epoch: 20: 0.7706054245170794\n",
            "epoch 21\n",
            "epoch 22\n",
            "epoch 23\n",
            "epoch 24\n",
            "epoch 25\n",
            "Train Loss after epoch: 25: 0.7100305646997157\n",
            "Validation loss after epoch: 25: 0.7662129964512459\n",
            "epoch 26\n",
            "epoch 27\n",
            "epoch 28\n",
            "epoch 29\n",
            "epoch 30\n",
            "Train Loss after epoch: 30: 0.7016984004121486\n",
            "Validation loss after epoch: 30: 0.7519390104051686\n",
            "epoch 31\n",
            "epoch 32\n",
            "epoch 33\n",
            "epoch 34\n",
            "epoch 35\n",
            "Train Loss after epoch: 35: 0.6740175072255173\n",
            "Validation loss after epoch: 35: 0.7541472000336786\n",
            "epoch 36\n",
            "epoch 37\n",
            "epoch 38\n",
            "epoch 39\n",
            "epoch 40\n",
            "Train Loss after epoch: 40: 0.6429730967002186\n",
            "Validation loss after epoch: 40: 0.7454337778902426\n",
            "epoch 41\n",
            "epoch 42\n",
            "epoch 43\n",
            "epoch 44\n",
            "epoch 45\n",
            "Train Loss after epoch: 45: 0.6245168464939769\n",
            "Validation loss after epoch: 45: 0.7381460388675768\n",
            "epoch 46\n",
            "epoch 47\n",
            "epoch 48\n",
            "epoch 49\n",
            "epoch 50\n",
            "Train Loss after epoch: 50: 0.603569877690416\n",
            "Validation loss after epoch: 50: 0.7328826838243775\n",
            "epoch 51\n",
            "epoch 52\n",
            "epoch 53\n",
            "epoch 54\n",
            "epoch 55\n",
            "Train Loss after epoch: 55: 0.571074609533558\n",
            "Validation loss after epoch: 55: 0.7578087696700301\n",
            "epoch 56\n",
            "epoch 57\n",
            "epoch 58\n",
            "epoch 59\n",
            "epoch 60\n",
            "Train Loss after epoch: 60: 0.5601156356858044\n",
            "Validation loss after epoch: 60: 0.7297635296864352\n",
            "epoch 61\n",
            "epoch 62\n",
            "epoch 63\n",
            "epoch 64\n",
            "epoch 65\n",
            "Train Loss after epoch: 65: 0.5298480254653992\n",
            "Validation loss after epoch: 65: 0.7323870601070787\n",
            "epoch 66\n",
            "epoch 67\n",
            "epoch 68\n",
            "epoch 69\n",
            "epoch 70\n",
            "Train Loss after epoch: 70: 0.5339870137509292\n",
            "Validation loss after epoch: 70: 0.7475755559334978\n",
            "epoch 71\n",
            "epoch 72\n",
            "epoch 73\n",
            "epoch 74\n",
            "epoch 75\n",
            "Train Loss after epoch: 75: 0.5111478645820928\n",
            "Validation loss after epoch: 75: 0.7507389101978631\n",
            "epoch 76\n",
            "epoch 77\n",
            "epoch 78\n",
            "epoch 79\n",
            "epoch 80\n",
            "Train Loss after epoch: 80: 0.49981725041943836\n",
            "Validation loss after epoch: 80: 0.7343606762805878\n",
            "epoch 81\n",
            "epoch 82\n",
            "epoch 83\n",
            "epoch 84\n",
            "epoch 85\n",
            "Train Loss after epoch: 85: 0.4571246435874846\n",
            "Validation loss after epoch: 85: 0.7460000586730463\n",
            "epoch 86\n",
            "epoch 87\n",
            "epoch 88\n",
            "epoch 89\n",
            "epoch 90\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}